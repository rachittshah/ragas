import argparse
import csv
import json
import os
from pathlib import Path
from typing import Dict, List

import dspy
import pandas as pd
from tqdm import tqdm

# -----------------------------------------------------------------------------
# Configuration helpers
# -----------------------------------------------------------------------------

def configure_lm(model: str = "openai/gpt-4o-mini") -> None:
    """Configure DSPy to use a particular LM via dspy.LM()."""
    if "OPENAI_API_KEY" not in os.environ:
        raise RuntimeError("OPENAI_API_KEY environment variable is required")

    lm = dspy.LM(model)
    dspy.configure(lm=lm)


# -----------------------------------------------------------------------------
# Judge module definition (DSPy)
# -----------------------------------------------------------------------------

class JudgeSignature(dspy.Signature):
    """LLM-as-judge prompt IO definition."""

    question: str = dspy.InputField(desc="the original user question")
    reference: str = dspy.InputField(desc="the gold-standard answer from dataset")
    candidate: str = dspy.InputField(desc="the answer generated by baseline system")

    score: int = dspy.OutputField(
        desc="integer quality rating 1 (very poor) … 5 (perfect)"
    )
    rationale: str = dspy.OutputField(desc="short explanation of the score")


class LLMJudge(dspy.Module):
    """A simple DSPy Chain-of-Thought judge that produces an ordinal score."""

    def __init__(self):
        super().__init__()
        self.evaluate = dspy.ChainOfThought(JudgeSignature, n=1)

    def forward(self, question: str, reference: str, candidate: str):  # type: ignore[override]
        pred = self.evaluate(question=question, reference=reference, candidate=candidate)
        # Basic sanitisation – ensure numeric score in range 1-5.
        try:
            score_int = int(str(pred.score).strip())
        except Exception:  # pragma: no cover – LLM may output weird
            score_int = 0
        score_int = max(1, min(5, score_int)) if score_int else 0
        return dspy.Prediction(score=score_int, rationale=pred.rationale)


# -----------------------------------------------------------------------------
# Main evaluation routine
# -----------------------------------------------------------------------------

def load_records(path: Path) -> List[Dict]:
    with open(path) as f:
        return [json.loads(line) for line in f]


def main(input_jsonl: Path, output_csv: Path, model_name: str):
    configure_lm(model_name)

    judge = LLMJudge()

    rows: List[Dict] = []
    for rec in tqdm(load_records(input_jsonl), desc="judging"):
        result = judge(question=rec["question"], reference=rec["reference"], candidate=rec["candidate"])
        rows.append(
            {
                "question": rec["question"],
                "reference": rec["reference"],
                "candidate": rec["candidate"],
                "score": result.score,
                "rationale": result.rationale,
            }
        )

    df = pd.DataFrame(rows)
    output_csv.parent.mkdir(parents=True, exist_ok=True)
    df.to_csv(output_csv, index=False, quoting=csv.QUOTE_MINIMAL)
    print(f"[evaluate_dspy] Wrote {len(df)} scored rows → {output_csv}")
    print(df["score"].describe())


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Evaluate answers with DSPy LLM judge")
    parser.add_argument("--input", type=Path, required=True, help="path to answers.jsonl")
    parser.add_argument(
        "--output", type=Path, default=Path("outputs/dspy_scores.csv"), help="output CSV path"
    )
    parser.add_argument(
        "--model", default="openai/gpt-4o-mini", help="dspy-compatible model name"
    )
    args = parser.parse_args()

    main(Path(args.input), Path(args.output), args.model)